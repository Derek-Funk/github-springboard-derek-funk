{"cells":[{"cell_type":"code","source":["# PART 1 - SET UP CONNECTIONS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"853932ad-0215-4fd3-ae23-eb93d68800d4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# packages used\nimport datetime\nimport math\nfrom multiprocessing.pool import ThreadPool\nimport numpy as np\nimport os\nimport pandas as pd\nfrom pyspark.sql import functions as F\nimport h5py # first install on cluster\nimport string # first install on cluster\nimport s3fs # first install on cluster\nimport threading"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"352151a6-2998-4639-9d7a-88cabb1d3156"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# S3 CONFIGURATIONS\n# read s3 bucket directly from databricks cluster: https://docs.databricks.com/data/data-sources/aws/amazon-s3.html\naccess_key = '<access_key>'\nsecret_key = '<secret_key>'\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\naws_bucket_name = \"bucket-msd-subset\"\npath_to_bucket = 's3a://{}/'.format(aws_bucket_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f57d4359-e899-425d-957b-c88d0f6d0c04"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# mounting the folder with sqlite DBs onto DBFS because I wasn't able to access directly\n# folder_to_mount = path_to_bucket + '/AdditionalFiles'\n# try:\n#   dbutils.fs.mount(folder_to_mount, '/mnt/temp/')\n# except:\n#   print('Folder may already be mounted.')\n# display(dbutils.fs.ls('/mnt/temp/'))\n# dbutils.fs.unmount('/mnt/temp/')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"560e9108-d0de-4f44-918e-8b0ed5c98029"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# AZURE SQL DATABASE CONFIGURATIONS\n# https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/sql-databases#python-example\njdbcHostname = 'azure-sql-server-msd.database.windows.net'\njdbcPort = 1433\njdbcDatabase = 'msd-subset'\njdbcUsername = 'derekfunk'\njdbcPassword = '<password>'\n\n# unable to connect to just the server level and create databases from there, only have been able to connect directly to pre-existing databases\n\njdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\nconnectionProperties = {\n  \"user\" : jdbcUsername,\n  \"password\" : jdbcPassword,\n  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n}\n\n# pushdown_query = \"(SELECT * FROM dbo.Persons) test\"\n# df = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)\n# display(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4cc40365-2ea8-4d70-a8c8-0c4e278470b0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# PART 2 - CREATE DATAFRAMES FROM FOLDER Additional Files (non-track data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c074b5a-942c-46db-b787-8d3e2e13febd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 1 - msd_artist\n# skipping lat/long b/c join is complicated\npath_to_file_msd_artist = path_to_bucket + 'AdditionalFiles/unique_artists.txt'\ndf_msd_artist = spark.read.format('csv') \\\n  .option('delimiter', '<SEP>') \\\n  .load(path_to_file_msd_artist) \\\n  .select(F.col('_c0').alias('artist_id'), F.col('_c1').alias('artist_mbid'), F.col('_c3').alias('artist_name'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76d63945-c4bc-4b5c-b8a2-f81ead8777bd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 2 - msd_artist_similarity\n# skipping since sqlite"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"edb9d69f-c068-438a-bde0-d5ceb22b777e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 3 - msd_r_term\npath_to_file_msd_r_term = path_to_bucket + 'AdditionalFiles/unique_terms.txt'\ndf_msd_r_term = spark.read.text(path_to_file_msd_r_term) \\\n  .withColumn('id', F.monotonically_increasing_id() + 1) \\\n  .select('id', 'value')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5bcf7151-435a-4f35-ba08-74ad137a843e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 4 - msd_r_mbtag\nbad_values = ['1 13 165900 150 7672 22647 34612 48720 59280 74602 87545 95495 107182 131087 141522 153710',\n'1 7 186240 183 23558 41608 89158 111733 150833 169883',\n              'ਪੰਜਾਬੀ',\n              'ਭੰਗੜਾ',\n              '香港歌手'\n             ]\n\npath_to_file_msd_r_mbtag = path_to_bucket + 'AdditionalFiles/unique_mbtags.txt'\ndf_msd_r_mbtag = spark.read.text(path_to_file_msd_r_mbtag) \\\n  .where(~(F.col('value').isin(bad_values))) \\\n  .withColumn('mbtag_id', F.monotonically_increasing_id() + 1) \\\n  .select('mbtag_id', F.col('value').alias('mbtag_name'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6372097c-e6d0-41ba-a25a-8a23684cfdcb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 5 - msd_artist_term\n# skipping since sqlite"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b3b5338-48a9-407d-b84d-eecc31e94f94"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 6 - msd_artist_mbtag\n# skipping since sqlite"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"018d9e62-b3ec-447f-b3bb-895d9a528d75"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# write all non-track tables\ntables_non_track = {\n  'msd_artist': df_msd_artist,\n  'msd_r_term': df_msd_r_term,\n  'msd_r_mbtag': df_msd_r_mbtag\n}\nfor table_name in tables_non_track.keys():\n  tables_non_track[table_name].write \\\n    .format('jdbc') \\\n    .option('url', jdbcUrl) \\\n    .option('dbtable', table_name) \\\n    .option('user', jdbcUsername) \\\n    .option('password', jdbcPassword) \\\n    .save()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4849eef4-9e5f-4d0b-8400-bec9267dbec4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# PART 3 - CREATE DATAFRAMES FROM FOLDER data (track data)\n# run this in DB to start over: drop table if exists msd_artist, msd_r_mbtag, msd_r_term, msd_track, msd_bar, msd_beat, msd_section, msd_tatum, msd_segment\n# run these to clear data but keep structure:\n# delete from msd_track"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d6dac4a-ab09-4650-a31b-8ce50d97ef76"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# create all track tables without data\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, TimestampType\n\nschema_msd_track = StructType([\n  StructField('track_id', StringType()),\n  StructField('analysis_sample_rate', IntegerType()),\n  StructField('audio_md5', StringType()),\n  StructField('danceability', FloatType()),\n  StructField('duration', FloatType()),\n  StructField('end_of_fade_in', FloatType()),\n  StructField('energy', FloatType()),\n  StructField('key', IntegerType()),\n  StructField('key_confidence', FloatType()),\n  StructField('loudness', FloatType()),\n  StructField('mode', IntegerType()),\n  StructField('mode_confidence', FloatType()),\n  StructField('start_of_fade_out', FloatType()),\n  StructField('tempo', FloatType()),\n  StructField('time_signature', IntegerType()),\n  StructField('time_signature_confidence', FloatType()),\n  StructField('artist_id', StringType()),\n  StructField('release', StringType()),\n  StructField('song_hotness', FloatType()),\n  StructField('title', StringType()),\n  StructField('track_7digitalid', StringType()),\n  StructField('year', IntegerType())\n#   ,\n#   StructField('upload_timestamp', TimestampType())\n])\n\nschema_msd_bar = StructType([\n  StructField('bar_id', IntegerType()),\n  StructField('track_id', StringType()),\n  StructField('track_bar_id', IntegerType()),\n  StructField('bar_confidence', FloatType()),\n  StructField('bar_start', FloatType())\n])\n\nschema_msd_beat = StructType([\n  StructField('beat_id', IntegerType()),\n  StructField('track_id', StringType()),\n  StructField('track_beat_id', IntegerType()),\n  StructField('beat_confidence', FloatType()),\n  StructField('beat_start', FloatType())\n])\n\nschema_msd_section = StructType([\n  StructField('section_id', IntegerType()),\n  StructField('track_id', StringType()),\n  StructField('track_section_id', IntegerType()),\n  StructField('section_confidence', FloatType()),\n  StructField('section_start', FloatType())\n])\n\nschema_msd_tatum = StructType([\n  StructField('tatum_id', IntegerType()),\n  StructField('track_id', StringType()),\n  StructField('track_tatum_id', IntegerType()),\n  StructField('tatum_confidence', FloatType()),\n  StructField('tatum_start', FloatType())\n])\n\nschema_msd_segment = StructType([\n  StructField('segment_id', IntegerType()),\n  StructField('track_id', StringType()),\n  StructField('track_segment_id', IntegerType()),\n  StructField('segment_confidence', FloatType()),\n  StructField('segment_start', FloatType()),\n  StructField('segment_loudness_max', FloatType()),\n  StructField('segment_loudness_max_time', FloatType()),\n  StructField('segment_loudness_start', FloatType()),\n  StructField('p1', FloatType()),\n  StructField('p2', FloatType()),\n  StructField('p3', FloatType()),\n  StructField('p4', FloatType()),\n  StructField('p5', FloatType()),\n  StructField('p6', FloatType()),\n  StructField('p7', FloatType()),\n  StructField('p8', FloatType()),\n  StructField('p9', FloatType()),\n  StructField('p10', FloatType()),\n  StructField('p11', FloatType()),\n  StructField('p12', FloatType()),\n  StructField('t1', FloatType()),\n  StructField('t2', FloatType()),\n  StructField('t3', FloatType()),\n  StructField('t4', FloatType()),\n  StructField('t5', FloatType()),\n  StructField('t6', FloatType()),\n  StructField('t7', FloatType()),\n  StructField('t8', FloatType()),\n  StructField('t9', FloatType()),\n  StructField('t10', FloatType()),\n  StructField('t11', FloatType()),\n  StructField('t12', FloatType())\n])\n\ntrack_schemas = {\n  'msd_track': schema_msd_track,\n  'msd_bar': schema_msd_bar,\n  'msd_beat': schema_msd_beat,\n  'msd_section': schema_msd_section,\n  'msd_tatum': schema_msd_tatum,\n  'msd_segment': schema_msd_segment\n}\n\nfor table_name in track_schemas.keys():\n  spark.createDataFrame([], track_schemas[table_name]).write \\\n    .format('jdbc') \\\n    .option('url', jdbcUrl) \\\n    .option('dbtable', table_name) \\\n    .option('user', jdbcUsername) \\\n    .option('password', jdbcPassword) \\\n    .save()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"babcd5a1-5ac9-4914-bba9-5f08b3a6e89e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# insert track data\nalphabet = tuple(string.ascii_uppercase)\nfolder_group_1 = [f'{path_to_bucket}data/A/{level_2}/{level_3}/' for level_2 in alphabet for level_3 in alphabet]\nfolder_group_2 = [f'{path_to_bucket}data/B/{level_2}/{level_3}/' for level_2 in alphabet[:8] for level_3 in alphabet]\nfolder_group_3 = [f'{path_to_bucket}data/B/I/{level_3}/' for level_3 in alphabet[:10]]\nfolders = folder_group_1 + folder_group_2 + folder_group_3\ns3 = s3fs.S3FileSystem(anon=False, key=access_key, secret=secret_key)\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71bb2cf4-ce62-4749-801f-b965fd59eed5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def write_folder(folder):\n  data_msd_track = None\n  data_msd_bar = None\n  data_msd_beat = None\n  data_msd_section = None\n  data_msd_tatum = None\n  data_msd_segment = None\n  \n#   for folder in list_of_folders:\n  \n  current_list_of_files = s3.ls(folder)\n\n  # loop over files in this folder\n  for file in current_list_of_files:\n  # file = current_list_of_files[0]\n\n    url_to_current_file = 's3a://' + file\n    h5 = h5py.File(s3.open(url_to_current_file), 'r')\n\n    # top level\n    track_id = h5['analysis/songs'][0][-1]\n\n    # h5['analysis'].keys()\n    analysis_sample_rate = np.array(h5['analysis/songs'])[0][0]\n    audio_md5 = np.array(h5['analysis/songs'])[0][1]\n    if np.array(h5['analysis/songs'])[0][2] == 0:\n        danceability = None\n    else:\n        danceability = np.array(h5['analysis/songs'])[0][2]\n    duration = np.array(h5['analysis/songs'])[0][3]\n    end_of_fade_in = np.array(h5['analysis/songs'])[0][4]\n    if np.array(h5['analysis/songs'])[0][5] == 0:\n        energy = None\n    else:\n        energy = np.array(h5['analysis/songs'])[0][5]\n    key = np.array(h5['analysis/songs'])[0][21]\n    key_confidence = np.array(h5['analysis/songs'])[0][22]\n    loudness = float(np.array(h5['analysis/songs'])[0][23])\n    mode = np.array(h5['analysis/songs'])[0][24]\n    mode_confidence = np.array(h5['analysis/songs'])[0][25]\n    start_of_fade_out = np.array(h5['analysis/songs'])[0][26]\n    tempo = np.array(h5['analysis/songs'])[0][27]\n    time_signature = np.array(h5['analysis/songs'])[0][28]\n    time_signature_confidence = np.array(h5['analysis/songs'])[0][29]\n\n    # h5['metadata'].keys()\n    artist_id = np.array(h5['metadata/songs'])[0][4]\n    release = np.array(h5['metadata/songs'])[0][14]\n    if math.isnan(np.array(h5['metadata/songs'])[0][16]):\n        song_hotness = None\n    else:\n        song_hotness = np.array(h5['metadata/songs'])[0][16]\n    title = np.array(h5['metadata/songs'])[0][18]\n    track_7digitalid = np.array(h5['metadata/songs'])[0][19]\n\n    # h5['musicbrainz'].keys()\n    if np.array(h5['musicbrainz/songs'])[0][1] == 0:\n        year = None\n    else:\n        year = np.array(h5['musicbrainz/songs'])[0][1]\n\n    #   upload_timestamp = datetime.datetime.now().replace(microsecond=0)\n    #   upload_timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    #   upload_timestamp = None\n    #   upload_timestamp = '2021-01-01 00:00:00'\n\n    data_msd_track_batch = np.column_stack((\n                track_id.decode(\"utf-8\"),\n                analysis_sample_rate,\n                audio_md5.decode(\"utf-8\"),\n                danceability,\n                duration,\n                end_of_fade_in,\n                energy,\n                key,\n                key_confidence,\n                loudness,\n                mode,\n                mode_confidence,\n                start_of_fade_out,\n                tempo,\n                time_signature,\n                time_signature_confidence,\n                artist_id.decode(\"utf-8\"),\n                release.decode(\"utf-8\"),\n                song_hotness,\n                title.decode(\"utf-8\"),\n                track_7digitalid,\n                year\n    #     ,\n    #               upload_timestamp\n            ))\n\n    if data_msd_track is None:\n        data_msd_track = data_msd_track_batch\n    else:\n        data_msd_track = np.row_stack((\n            data_msd_track,\n            data_msd_track_batch\n        ))\n\n    #   bar\n    bars_confidence = np.array(h5['analysis/bars_confidence'])\n    no_bars = len(bars_confidence)\n    bars_start = np.array(h5['analysis/bars_start'])\n    data_msd_bar_batch = np.column_stack((\n        np.full((no_bars), None),\n        np.full((no_bars), track_id.decode(\"utf-8\")),\n        np.array(range(1, no_bars + 1)),\n        bars_confidence,\n        bars_start\n    ))\n    if data_msd_bar is None:\n        data_msd_bar = data_msd_bar_batch\n    else:\n        data_msd_bar = np.row_stack((\n            data_msd_bar,\n            data_msd_bar_batch\n        ))\n\n    # beat\n    beats_confidence = np.array(h5['analysis/beats_confidence'])\n    no_beats = len(beats_confidence)\n    beats_start = np.array(h5['analysis/beats_start'])\n    data_msd_beat_batch = np.column_stack((\n        np.full((no_beats), None),\n        np.full((no_beats), track_id.decode(\"utf-8\")),\n        np.array(range(1, no_beats + 1)),\n        beats_confidence,\n        beats_start\n    ))\n    if data_msd_beat is None:\n        data_msd_beat = data_msd_beat_batch\n    else:\n        data_msd_beat = np.row_stack((\n            data_msd_beat,\n            data_msd_beat_batch\n        ))\n\n    # section\n    sections_confidence = np.array(h5['analysis/sections_confidence'])\n    no_sections = len(sections_confidence)\n    sections_start = np.array(h5['analysis/sections_start'])\n    data_msd_section_batch = np.column_stack((\n        np.full((no_sections), None),\n        np.full((no_sections), track_id.decode(\"utf-8\")),\n        np.array(range(1, no_sections + 1)),\n        sections_confidence,\n        sections_start\n    ))\n    if data_msd_section is None:\n        data_msd_section = data_msd_section_batch\n    else:\n        data_msd_section = np.row_stack((\n            data_msd_section,\n            data_msd_section_batch\n        ))\n\n    # tatum\n    tatums_confidence = np.array(h5['analysis/tatums_confidence'])\n    no_tatums = len(tatums_confidence)\n    tatums_start = np.array(h5['analysis/tatums_start'])\n    data_msd_tatum_batch = np.column_stack((\n        np.full((no_tatums), None),\n        np.full((no_tatums), track_id.decode(\"utf-8\")),\n        np.array(range(1, no_tatums + 1)),\n        tatums_confidence,\n        tatums_start\n    ))\n    if data_msd_tatum is None:\n        data_msd_tatum = data_msd_tatum_batch\n    else:\n        data_msd_tatum = np.row_stack((\n            data_msd_tatum,\n            data_msd_tatum_batch\n        ))\n\n    # segment\n    segments_confidence = np.array(h5['analysis/segments_confidence'])\n    no_segments = len(segments_confidence)\n    segments_start = np.array(h5['analysis/segments_start'])\n    segments_loudness_max = np.array(h5['analysis/segments_loudness_max'])\n    segments_loudness_max_time = np.array(h5['analysis/segments_loudness_max_time'])\n    segments_loudness_start = np.array(h5['analysis/segments_loudness_start'])\n    segments_pitch = np.array(h5['analysis/segments_pitches'])\n    segments_timbre = np.array(h5['analysis/segments_timbre'])\n\n    data_msd_segment_batch = np.column_stack((\n        np.full((no_segments), None),\n        np.full((no_segments), track_id.decode(\"utf-8\")),\n        np.array(range(1, no_segments + 1)),\n        segments_confidence,\n        segments_start,\n        segments_loudness_max,\n        segments_loudness_max_time,\n        segments_loudness_start,\n        segments_pitch,\n        segments_timbre\n    ))\n    if data_msd_segment is None:\n        data_msd_segment = data_msd_segment_batch\n    else:\n        data_msd_segment = np.row_stack((\n            data_msd_segment,\n            data_msd_segment_batch\n        ))\n          \n  spark.createDataFrame(pd.DataFrame(data_msd_track), schema=schema_msd_track).write \\\n        .mode('append') \\\n        .format('jdbc') \\\n        .option('url', jdbcUrl) \\\n        .option('dbtable', 'msd_track') \\\n        .option('user', jdbcUsername) \\\n        .option('password', jdbcPassword) \\\n        .save()    \n  spark.createDataFrame(pd.DataFrame(data_msd_bar), schema=schema_msd_bar).write \\\n        .mode('append') \\\n        .format('jdbc') \\\n        .option('url', jdbcUrl) \\\n        .option('dbtable', 'msd_bar') \\\n        .option('user', jdbcUsername) \\\n        .option('password', jdbcPassword) \\\n        .save() \n  spark.createDataFrame(pd.DataFrame(data_msd_beat), schema=schema_msd_beat).write \\\n        .mode('append') \\\n        .format('jdbc') \\\n        .option('url', jdbcUrl) \\\n        .option('dbtable', 'msd_beat') \\\n        .option('user', jdbcUsername) \\\n        .option('password', jdbcPassword) \\\n        .save() \n  spark.createDataFrame(pd.DataFrame(data_msd_section), schema=schema_msd_section).write \\\n        .mode('append') \\\n        .format('jdbc') \\\n        .option('url', jdbcUrl) \\\n        .option('dbtable', 'msd_section') \\\n        .option('user', jdbcUsername) \\\n        .option('password', jdbcPassword) \\\n        .save() \n  spark.createDataFrame(pd.DataFrame(data_msd_tatum), schema=schema_msd_tatum).write \\\n        .mode('append') \\\n        .format('jdbc') \\\n        .option('url', jdbcUrl) \\\n        .option('dbtable', 'msd_tatum') \\\n        .option('user', jdbcUsername) \\\n        .option('password', jdbcPassword) \\\n        .save() \n  spark.createDataFrame(pd.DataFrame(data_msd_segment), schema=schema_msd_segment).write \\\n        .mode('append') \\\n        .format('jdbc') \\\n        .option('url', jdbcUrl) \\\n        .option('dbtable', 'msd_segment') \\\n        .option('user', jdbcUsername) \\\n        .option('password', jdbcPassword) \\\n        .save()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"432ed11e-ee52-42c6-9095-32dc9a5a418e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from multiprocessing.pool import ThreadPool\npool = ThreadPool(1000)\npool.map(lambda x: write_folder(x), folders)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b51b7ea-ce33-48d3-9e94-a338bbc8432a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Cancelled","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"s3_to_azure_mysql_subset","dashboards":[],"language":"python","widgets":{},"notebookOrigID":1921775291071983}},"nbformat":4,"nbformat_minor":0}
